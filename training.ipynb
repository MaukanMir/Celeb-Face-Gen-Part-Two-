{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "  model = Sequential()\n",
    "  # foundation for 4x4 image\n",
    "  n_nodes = 256 * 4 * 4\n",
    "  \n",
    "  model.add(Dense(n_nodes, input_dim=latent_dim)) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Reshape((4, 4, 256)))\n",
    "  # upsample to 8x8\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  # upsample to 32x32\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same')) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "  # make weights in the discriminator not trainable \n",
    "  d_model.trainable = False\n",
    "  # connect them\n",
    "  model = Sequential()\n",
    "  # add generator\n",
    "  model.add(g_model)\n",
    "  # add the discriminator\n",
    "  model.add(d_model)\n",
    "  # compile model\n",
    "  opt = Adam(learning_rate=0.0002, beta_1=0.5) \n",
    "  model.compile(loss='binary_crossentropy', optimizer=opt) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fake Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "  # generate points in latent space\n",
    "  x_input = generate_latent_points(latent_dim, n_samples)\n",
    "  # predict outputs\n",
    "  X = g_model.predict(x_input)\n",
    "  # create ✬fake✬ class labels (0) \n",
    "  y = zeros((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Raal Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "  # choose random instances\n",
    "  ix = randint(0, dataset.shape[0], n_samples)\n",
    "  # retrieve selected images\n",
    "  X = dataset[ix]\n",
    "  # generate ✬real✬ class labels (1) \n",
    "  y = ones((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  x_input = x_input.reshape(n_samples, latent_dim)\n",
    "  return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "  # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "    # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "          # get randomly selected ✬real✬ samples\n",
    "          X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "          # update discriminator model weights\n",
    "          d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "          # generate ✬fake✬ examples\n",
    "          X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) # update discriminator model weights\n",
    "          d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "          # prepare points in latent space as input for the generator\n",
    "          X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "          # create inverted labels for the fake samples\n",
    "          y_gan = ones((n_batch, 1))\n",
    "          # update the generator via the discriminator✬s error\n",
    "          g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "#           # summarize loss on this batch\n",
    "#           print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "#                 (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
